# Regression and model validation

*This file describes the work and results of the second week a. k. a. "Regression and model validation" of the IDOS2023 course.*

### Information on data set

The data set includes data which is based on a survey conducted in Finland between 3.12.2014 and 10.1.2015. The aim of the survey was to find the relationship between learning approaches and students achievements in an introductory statistics course in Finland (click [here](https://www.slideshare.net/kimmovehkalahti/the-relationship-between-learning-approaches-and-students-achievements-in-an-introductory-statistics-course-in-finland) to see more information on the data set and the course). The dataset has 166 observations / rows with 7 variables / columns, of which four are numerical, two are integer and one is a categorical character variable (see output below for details). Note that only few of the originally recorded values are included in this data set.

```{r, message = FALSE}
# message = FALSE will not show any message from R

# Read and summarise file 
# load saved file from data wrangling exercise 
learning2014 <- read.table(file='/home/ntriches/github_iods2023/IODS23/data/learning2014.csv', header=TRUE, sep = ",")
str(learning2014) # structure of data
dim(learning2014) # dimension of data

```

The following figure shows a plot matrix of the 7 variables in the dataset, where each variable is plotted versus one other variable. The pink colour shows data from female students, whereas the blue colour shows data from male students of the survey. The scatter plots, distribution and correlation are therefore divided in both female and male students. The distribution of the data shows that the majority of the students were very young (< 25 years old), leading to a right skewed distribution of *age*. All other variables are relatively normally distributed, with some tendencies towards a left skewed distribution for *points* and *deep*, and the *male attitude* variables. This can be well seen in the outliers of the box plots on the top row of the matrix.

We can see that a better *attitude* towards the course, represented in a higher number, seems to lead to significantly higher *points*, in other words, to a better results in the exam (p < 0.001). It also appears that older male students might get less points in the exam (p < 0.1). The strategic learning approach (*stra*) might lead to better exam results, shown by a positive correlation (p < 0.1). On the other hand, we might assume that deep (*deep*) or surface learning (*surf*) are not successful learning approaches, as they show negative correlations with points (p < 0.1)


```{r, message = FALSE, fig.width=14, fig.height=12}
# fig. width and fig.height enlarge the figure below

# Graphical overview 
# load libraries 
library(ggplot2)
library(GGally)

# create plot matrix 
overview_plot <- learning2014 %>%
  ggpairs(mapping = aes(col = gender, alpha = 0.3),
          lower = list(combo = wrap("facethist", bins = 20))) +
  theme_grey(base_size = 20)
overview_plot

```



### Regression model 

For the multivariable regression model, I selected the three explanatory (predictor) variables *age*, *attitude*, and strategic learning (*stra*). The dependent target variable is **points**. For my model, I set the significance level at p = 0.05. 

The model summary as shown below first shows a reminder of how the model was fit in the function call, then a summary of the distribution of the residuals, the results ("Coefficients"), and an indication of the general model quality ("Residual standard error", "R-squared", "F-statistic"). In the table of the model coefficients, we can see the estimated value of the coefficient with its estimated standard error, and the corresponding t-statistic and p-value. In the last part of the output, we can see an estimate of the residual standard error with the corresponding degrees of freedom (166 obs. - 3 = 163). With the multiple R-squared-value, we can see how much of the variance of **points** has been explained by the model. The adjusted R-squared takes into account the number of variables included in the model. The final line shows us the results of the F-test testing the hypothesis that all coefficients except the intercept are equal to zero. 

In my model, *attitude* shows a positive relationship with **points** (p < 0.001). In other words, a positive attitude towards the course led to higher exam results: with an increase of 1 in attitude, the points in the exam are estimated to increase by 3.48 points. On the other hand, *age* and *stra* did not show any significant relationship with the exam results (p > 0.05). There are trends showing that points in the exams very slightly decrease with age but increase using the strategic learning approach, but they are not significant according to my definition. Overall, the explanatory variables I chose in my model only explain 21.8% of the variation in the data, as shown by the multiple R-squared of 0.2182. The fitted model has a residual standard error of 5.26 points. 

If I remove *age* and *stra* from my model, *attitude* remains highly significant (p < 0.001) but the multiple R-squared decreases slightly to 19%. If I run the model with *attitude* and both *stra* and *age* separately, the multiple R-squared increases to 20.48% and 20.11%, respectively, but none of the explanatory variables other than attitude have a p-value higher than 0.05. Overall, the first model including three explanatory variables seemed to best explain the data, and shows that the attitude has a significant influence on the points in the exam. 


```{r}
# Regression model 

# create a regression model with three explanatory variables (stra, age, attitude)
lm_model_points_stra_age_attitude <- lm(points ~ stra + age + attitude, data = learning2014)
# show summary of the fitted model 
summary(lm_model_points_stra_age_attitude)

# Does model (R-squared) improve with less explanatory variables? 
# create a simple linear regression with attitute 
lm_model_points_attitude <- lm(points ~ attitude, data = learning2014)
summary(lm_model_points_attitude) # no

# create a regression model with two explanatory variables (stra, attitude)
lm_model_points_stra_attitude <- lm(points ~ stra + attitude, data = learning2014)
summary(lm_model_points_stra_attitude)

# create a regression model with two explanatory variables (age, attitude)
lm_model_points_age_attitude <- lm(points ~ age + attitude, data = learning2014)
summary(lm_model_points_age_attitude)

```


### Assumptions of the model and validity interpretation 

The assumptions for linear regression are as follows:
1. Linearity, *i. e.*, there is a linear relation between the explanatory and target variable.
2. Homoscedasticity, *i. e.*, the variance of the target variable should be the same across the range of the explanatory variable.
3. Normality of the error terms, *i. e.*, the error terms should follow a normal distribution with mean zero. 

Because we have continuous variables, we can only assess the assumptions by using and looking at the residuals. The residuals are the difference between the observed and the fitted values. On the top left plot of the following figure ("Residuals vs Fitted"), we can see that the data is relatively normally distributed, so not deviating from the horizontal axis at Y = 0. Also the spread around the horizontal axis at Y = 0 is not deviating too much, so the assumption of homoscedasticity is fulfilled, too. The normality of the error terms can be seen in the top right plot of the following figure ("Normal Q-Q"). There are no major deviations from normality, so this assumption is also fulfilled. In the bottom left plot ("Residuals vs Leverage"), we can get an estimate on how much influence each observation had in the fitting of the regression model due to its explanatory variables. 

```{r, fig.width=8, fig.height=6}
# Plot Residuals vs Fitted values, Normal QQ-plot, and Residuals vs Leverage 

par(mfrow = c(2,2))
plot(lm_model_points_stra_age_attitude,
     which = c(1,2,5))
par(mfrow = c(1,1))
```
